{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets  # Interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def ex3_plot(model, x, y, ep, lss):\n",
    "  \"\"\"\n",
    "  Plot training loss\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Model implementing regression\n",
    "    x: np.ndarray\n",
    "      Training Data\n",
    "    y: np.ndarray\n",
    "      Targets\n",
    "    ep: int\n",
    "      Number of epochs\n",
    "    lss: function\n",
    "      Loss function\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "  ax1.set_title(\"Regression\")\n",
    "  ax1.plot(x, model(x).detach().numpy(), color='r', label='prediction')\n",
    "  ax1.scatter(x, y, c='c', label='targets')\n",
    "  ax1.set_xlabel('x')\n",
    "  ax1.set_ylabel('y')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.set_title(\"Training loss\")\n",
    "  ax2.plot(np.linspace(1, epochs, epochs), losses, color='y')\n",
    "  ax2.set_xlabel(\"Epoch\")\n",
    "  ax2.set_ylabel(\"MSE\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def ex1_plot(fun_z, fun_dz):\n",
    "  \"\"\"\n",
    "  Plots the function and gradient vectors\n",
    "\n",
    "  Args:\n",
    "    fun_z: f.__name__\n",
    "      Function implementing sine function\n",
    "    fun_dz: f.__name__\n",
    "      Function implementing sine function as gradient vector\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  x, y = np.arange(-3, 3.01, 0.02), np.arange(-3, 3.01, 0.02)\n",
    "  xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "  zz = fun_z(xx, yy)\n",
    "  xg, yg = np.arange(-2.5, 2.6, 0.5), np.arange(-2.5, 2.6, 0.5)\n",
    "  xxg, yyg = np.meshgrid(xg, yg, sparse=True)\n",
    "  zxg, zyg = fun_dz(xxg, yyg)\n",
    "\n",
    "  plt.figure(figsize=(8, 7))\n",
    "  plt.title(\"Gradient vectors point towards steepest ascent\")\n",
    "  contplt = plt.contourf(x, y, zz, levels=20)\n",
    "  plt.quiver(xxg, yyg, zxg, zyg, scale=50, color='r', )\n",
    "  plt.xlabel('$x$')\n",
    "  plt.ylabel('$y$')\n",
    "  ax = plt.gca()\n",
    "  divider = make_axes_locatable(ax)\n",
    "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "  cbar = plt.colorbar(contplt, cax=cax)\n",
    "  cbar.set_label('$z = h(x, y)$')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
