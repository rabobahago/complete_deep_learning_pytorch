{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor,Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    def train_loop(self, dataloader, model, loss_fn, optimizer):\n",
    "        size = len(dataloader.dataset)\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "    def test_loop(self, dataloader, model, loss_fn):\n",
    "        size = len(dataloader.dataset)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= size\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001F300208E80>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001F3002090C0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1043, 0.0718, 0.1687],\n",
      "        [0.1043, 0.0718, 0.1687],\n",
      "        [0.1043, 0.0718, 0.1687],\n",
      "        [0.1043, 0.0718, 0.1687],\n",
      "        [0.1043, 0.0718, 0.1687]])\n",
      "tensor([0.1043, 0.0718, 0.1687])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298276  [    0/60000]\n",
      "loss: 2.292351  [ 6400/60000]\n",
      "loss: 2.279995  [12800/60000]\n",
      "loss: 2.285454  [19200/60000]\n",
      "loss: 2.269860  [25600/60000]\n",
      "loss: 2.250703  [32000/60000]\n",
      "loss: 2.268472  [38400/60000]\n",
      "loss: 2.243649  [44800/60000]\n",
      "loss: 2.229423  [51200/60000]\n",
      "loss: 2.227210  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 0.034981 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.207881  [    0/60000]\n",
      "loss: 2.208300  [ 6400/60000]\n",
      "loss: 2.187683  [12800/60000]\n",
      "loss: 2.228541  [19200/60000]\n",
      "loss: 2.173509  [25600/60000]\n",
      "loss: 2.143873  [32000/60000]\n",
      "loss: 2.187097  [38400/60000]\n",
      "loss: 2.139836  [44800/60000]\n",
      "loss: 2.119034  [51200/60000]\n",
      "loss: 2.114099  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 0.033358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.083110  [    0/60000]\n",
      "loss: 2.083461  [ 6400/60000]\n",
      "loss: 2.051262  [12800/60000]\n",
      "loss: 2.141989  [19200/60000]\n",
      "loss: 2.015906  [25600/60000]\n",
      "loss: 1.984955  [32000/60000]\n",
      "loss: 2.055539  [38400/60000]\n",
      "loss: 1.984807  [44800/60000]\n",
      "loss: 1.965034  [51200/60000]\n",
      "loss: 1.943373  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 0.031082 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.911289  [    0/60000]\n",
      "loss: 1.914167  [ 6400/60000]\n",
      "loss: 1.879440  [12800/60000]\n",
      "loss: 2.029025  [19200/60000]\n",
      "loss: 1.826607  [25600/60000]\n",
      "loss: 1.810209  [32000/60000]\n",
      "loss: 1.901218  [38400/60000]\n",
      "loss: 1.829273  [44800/60000]\n",
      "loss: 1.804017  [51200/60000]\n",
      "loss: 1.767231  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 0.028877 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.741050  [    0/60000]\n",
      "loss: 1.759748  [ 6400/60000]\n",
      "loss: 1.731094  [12800/60000]\n",
      "loss: 1.924994  [19200/60000]\n",
      "loss: 1.675464  [25600/60000]\n",
      "loss: 1.677662  [32000/60000]\n",
      "loss: 1.767584  [38400/60000]\n",
      "loss: 1.712885  [44800/60000]\n",
      "loss: 1.675292  [51200/60000]\n",
      "loss: 1.637061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 0.027138 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.604291  [    0/60000]\n",
      "loss: 1.639213  [ 6400/60000]\n",
      "loss: 1.614066  [12800/60000]\n",
      "loss: 1.843283  [19200/60000]\n",
      "loss: 1.563240  [25600/60000]\n",
      "loss: 1.581470  [32000/60000]\n",
      "loss: 1.668501  [38400/60000]\n",
      "loss: 1.627919  [44800/60000]\n",
      "loss: 1.582286  [51200/60000]\n",
      "loss: 1.549165  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 0.025810 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.502696  [    0/60000]\n",
      "loss: 1.548041  [ 6400/60000]\n",
      "loss: 1.520981  [12800/60000]\n",
      "loss: 1.780196  [19200/60000]\n",
      "loss: 1.480276  [25600/60000]\n",
      "loss: 1.510741  [32000/60000]\n",
      "loss: 1.598408  [38400/60000]\n",
      "loss: 1.563933  [44800/60000]\n",
      "loss: 1.514336  [51200/60000]\n",
      "loss: 1.488483  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 0.024791 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.426960  [    0/60000]\n",
      "loss: 1.476036  [ 6400/60000]\n",
      "loss: 1.444998  [12800/60000]\n",
      "loss: 1.731447  [19200/60000]\n",
      "loss: 1.420046  [25600/60000]\n",
      "loss: 1.458574  [32000/60000]\n",
      "loss: 1.549336  [38400/60000]\n",
      "loss: 1.514558  [44800/60000]\n",
      "loss: 1.464013  [51200/60000]\n",
      "loss: 1.444280  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 0.024009 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.368311  [    0/60000]\n",
      "loss: 1.420168  [ 6400/60000]\n",
      "loss: 1.381842  [12800/60000]\n",
      "loss: 1.691382  [19200/60000]\n",
      "loss: 1.376410  [25600/60000]\n",
      "loss: 1.418574  [32000/60000]\n",
      "loss: 1.512860  [38400/60000]\n",
      "loss: 1.475898  [44800/60000]\n",
      "loss: 1.423865  [51200/60000]\n",
      "loss: 1.409239  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 0.023375 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.320899  [    0/60000]\n",
      "loss: 1.373941  [ 6400/60000]\n",
      "loss: 1.328447  [12800/60000]\n",
      "loss: 1.656885  [19200/60000]\n",
      "loss: 1.342696  [25600/60000]\n",
      "loss: 1.386311  [32000/60000]\n",
      "loss: 1.483122  [38400/60000]\n",
      "loss: 1.444223  [44800/60000]\n",
      "loss: 1.390086  [51200/60000]\n",
      "loss: 1.380131  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 0.022840 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
